# PEFT Adapter Configuration for LoRA

# Adapter type
adapter_type: "lora"  # Standard Low-Rank Adaptation

# Target modules to apply adapters to
target_modules:
  - "to_q"
  - "to_k"
  - "to_v"
  - "to_out.0"

# General adapter parameters
init_weights: true
module_dropout: 0.0
modules_to_save: null

# LoRA specific parameters
rank: 1              # LoRA rank
alpha: 1             # LoRA alpha for scaling
bias: "none"         # Bias type for LoRA. Can be 'none', 'all' or 'lora_only' 